{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLC0tjKOR6X2"
      },
      "source": [
        "# Web Scraping 101 (oDCM)\n",
        "\n",
        "*After finishing this tutorial, you can extract data from multiple pages on the web, and export such data to CSV files so that you can use it in an analysis. Plan a few hours to work through this notebook. Taking a few breaks inbetween keeps you sharp! Enjoy!*\n",
        "\n",
        "--- \n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "* Generate lists of entities to scrape data from\n",
        "* Map navigation path on a website using URLs, and understand how to use parameters to modify results\n",
        "* Select data for extraction on a website using tags, class names and attributes\n",
        "* Write data to CSV file, and enrich with relevant metadata\n",
        "* Bundle data capture in Python functions and modularize extraction code\n",
        "* Loop through a list of URLs to capture data in bulk, using functions\n",
        "* Understand the difference between Jupyter Notebooks and “raw” Python files, and run collection via the command line/terminal\n",
        "\n",
        "--- \n",
        "\n",
        "## Acknowledgements\n",
        "This tutorial has been inspired by various open-access online resources, which we list for further reference at the [course website](https://odcm.hannesdatta.com/docs/about). \n",
        "\n",
        "--- \n",
        "\n",
        "## Support Needed?\n",
        "For technical issues outside of scheduled classes, please check the [support section](https://odcm.hannesdatta.com/docs/course/support) on the course website. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2H0-moWR6YC"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. Seed Generation\n",
        "\n",
        "\n",
        "### 1.1 Collecting Links\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP7Y3DWUR6YD"
      },
      "source": [
        "__Importance__\n",
        "\n",
        "In web scraping, we typically refer to a \"seed\" as a starting point for a data collection. Without a seed, there's no data to collect.\n",
        "\n",
        "For example, before we can crawl through all books available on [this site](https://books.toscrape.com/catalogue/category/books_1/index.html), we first need to generate a *list of all books on the page*.\n",
        "\n",
        "One way to get there would be to:\n",
        "\n",
        "1. first scrape all book links (“seeds”) from the overview page, and \n",
        "2. then iterate over all links to scrape the product description (or anything else on that page). \n",
        "\n",
        "Note that the overview page allows us to \"navigate\" to the individual book pages, either by clicking on the book cover or the book title (see red boxes in the figure below). \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/books_links.png\" align=\"left\" width=80%/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "balaQYf9R6YE"
      },
      "source": [
        "__Let's try it out__\n",
        "\n",
        "Let's now check out how the links from the book covers or book titles are encoded in the website's source code.\n",
        "\n",
        "Open the [book catalogue](https://books.toscrape.com/catalogue/category/books_1/index.html), and inspect the underlying HTML code with the Chrome Inspector (right click --> inspect element). \n",
        "\n",
        "The book covers (`<img>`) are surrounded by `<a>` tags, which contain a link (`href`) to the book. \n",
        "\n",
        "Also, the book titles (`<h3>`) are surrounded by `<a>` tags with the relevant links to the book pages.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/inspector_links.png\" align=\"left\" width=80%/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bZQTQnDR6YF"
      },
      "source": [
        "How could we tell a computer to capture the links to the various books on the site?\n",
        "\n",
        "One simple way is to select *elements by their tags*. For example, to extract all links (`<a>` tags). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOkkytCmR6YF"
      },
      "source": [
        "__Exercise 1__\n",
        "\n",
        "Please run the code cell below, which extracts all links (the `a` tag!), and prints the URL (`href`) to the screen. Don't worry, you don't need need to understand the code yet, we'll go over it line by line shortly!\n",
        "\n",
        "If you look at these links more closely, you'll notice that we're not interested in many of these links... \n",
        "\n",
        "Make a list of all links we're *not* interested in (i.e., those *not* pointing to a book page). Which ones are those? Can you find out why they are there?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUq-gnF3R6YG"
      },
      "outputs": [],
      "source": [
        "# Run this code now\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
        "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
        "res = requests.get(url)\n",
        "soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "# return the href attribute in the <a> tag nested within the first product class element\n",
        "for link in soup.find_all(\"a\"): \n",
        "    print(link.attrs[\"href\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q59wBWgR6YI"
      },
      "source": [
        "**Your answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjCWJZvMR6YJ"
      },
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz6gPGWeR6YK"
      },
      "source": [
        "__Solution__\n",
        "\n",
        "The links we want to ignore are...\n",
        "\n",
        "* \"Books to Scrape\" link at the top\n",
        "* \"Home\" breadcrumb link \n",
        "* Left sidebar with all book genres (e.g., Travel)\n",
        "* The next button at the bottom\n",
        "\n",
        "These links are present on the page, because they are used by users to navigate on the page. This can also be seen on the animation:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/books_overview.gif\" align=\"left\" width=50%/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ad_lOIkR6YK"
      },
      "source": [
        "### 1.2 Collecting *More Specific* Links\n",
        "\n",
        "__Importance__\n",
        "\n",
        "We've just discovered that selecting elements by their tags gives us many irrelevant links. But, how can we narrow down these links, or, in other words, __how can we scrape only the book links we're interested in?__.\n",
        "\n",
        "To answer this question, we need to briefly revisit the notion of __HTML classes__. \n",
        "\n",
        "A __class__ is often used as a reference in the code. For example, to make all text elements with a given class blue or increase the font size. In the Google Inspector screenshot shown earlier, you find an `<article>` tag with class `product_pod` in which a `<div>` is nested which contains the image and link attribute we're after. \n",
        "\n",
        "Every link to a book is *nested within this class* (nested = \"part of\"). The \"wrong links\" extracted above (i.e., the ones in the page's header and sidebar) are *not*. \n",
        "\n",
        "Thus, if we can tell our scraper that we're only interested in the `<a>` tags *within the `product_pod` class*, we end up with our desired selection of links."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieo4p3J-R6YL"
      },
      "source": [
        "__Let's try it out__\n",
        "\n",
        "Like before, we'll use `.find_all()` to capture all matching elements on the page. The difference, however, is that we specify __a class (`class_=`)__, rather than an HTML tag. From the inspector, we know the class name (`product_pod`). \n",
        "\n",
        "This result is a list with __all 20 `product_pod` classes__ on the page (i.e., one for each book). \n",
        "\n",
        "Run the code below, in which we pick the __first book__ from the list (A Light in the Attic, element `[0]`), and extract the `<a>` tag nested within the `product_pod` class. \n",
        "\n",
        "Finally, we pull out the `href` attribute from the `<a>` tag which gives us the book link. Unlike the example above, we have selected only a single element (`[0]`) and therefore don't need to loop over all links with a `for`-loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2no41nNR6YM"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
        "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
        "res = requests.get(url)\n",
        "soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "# return the href attribute in the <a> tag nested within the first product class element\n",
        "soup.find_all(class_=\"product_pod\")[0].find(\"a\").attrs[\"href\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZu40yshR6YR"
      },
      "source": [
        "Note the `../../` in front of the link which tells the browser: this tells the browser to go back two directories from the current URL:\n",
        "* Current URL: https://books.toscrape.com/catalogue/category/books_1/index.html\n",
        "* 1 step back: https://books.toscrape.com/catalogue/category/books_1\n",
        "* 2 steps back: https://books.toscrape.com/catalogue/category/\n",
        "\n",
        "Thereafter, it appends `a-light-in-the-attic_1000/index.html` to the URL which forms the full link to the [A Light in the Attic](https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html) book. \n",
        "\n",
        "Pretty cool, right?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi61X2IdR6YR"
      },
      "source": [
        "#### Exercise 2\n",
        "1. Modify the script to extract the link from the *second book* (Tipping the Velvet), using BeautifulSoup.\n",
        "2. Create a new variable `book_url` that concatenates the base URL (` https://books.toscrape.com/catalogue/`) and the string you extracted in the previous exercise 1.2 (`../../a-light-....`). Use *slicing* to remove the `../../` part inbetween. The final output should be: `https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html` \n",
        "3. The `replace` functions offers a more convenient way to \"search and replace\" in a string. The syntax is: `my_string = old_string.replace('text-to-replace', 'replace-by-text')`. Implement the `replace` function for the previous exercise 2.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O58PnnuCR6YS"
      },
      "outputs": [],
      "source": [
        "# your answer goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04Btg2b5R6YS"
      },
      "source": [
        "#### Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQwhBhijR6YS"
      },
      "outputs": [],
      "source": [
        "# Question 1\n",
        "url_book = soup.find_all(class_=\"product_pod\")[1].find(\"a\").attrs[\"href\"]\n",
        "print(url_book)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPk1AlKsR6YS"
      },
      "outputs": [],
      "source": [
        "# Question 2 \n",
        "base_url = \"https://books.toscrape.com/catalogue/\" # gives a 403 error if you run the URL separately but works as expected once combined with the book url\n",
        "book_url = base_url + url_book[6:] # so we skip characters with index 0, 1, 2, 3, 4, 5: \"../../\"\n",
        "print(book_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSF4fMXmR6YT"
      },
      "outputs": [],
      "source": [
        "# Question 3\n",
        "base_url = \"https://books.toscrape.com/catalogue/\"\n",
        "book_url = base_url + url_book\n",
        "book_url = book_url.replace('../', '')\n",
        "print(book_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlmJo_nKR6YU"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV7T007DR6YV"
      },
      "source": [
        "### 1.3 Iterating over items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTfoJBNYR6YV"
      },
      "source": [
        "__Importance__\n",
        "\n",
        "Ideally, we'd like our code to extract the URL from *every* book on the page, not just *one* product.\n",
        "\n",
        "In other words, we need a way to *iterate*/*loop* through the entire page to assemble a list of links (product pages) to scrape.\n",
        "\n",
        "__Let's try it out__\n",
        "\n",
        "Let's set up this exercise.\n",
        "\n",
        "1. We have a BeautifulSoup object, holding all of the book previews (`soup.find_all(class_=\"product_pod\")`)\n",
        "2. We have an empty array of `book_urls`, that we would like to fill\n",
        "3. We write a loop, which iterates through 1. and fills in 2.\n",
        "\n",
        "Run the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwvQCbT4R6YW"
      },
      "outputs": [],
      "source": [
        "# list of all books on the overview page\n",
        "books = soup.find_all(class_=\"product_pod\")\n",
        "book_urls = []\n",
        "\n",
        "for book in books: \n",
        "    book_url = book.find(\"a\").attrs[\"href\"]\n",
        "    book_urls.append(book_url)\n",
        "    \n",
        "# print the first five urls\n",
        "print(book_urls[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUDltBV-R6YX"
      },
      "source": [
        "In practice, it may be more convenient to create a *dictionary* in which the `book_title` is the key and the `book_url` the value. This way it is more intuitive to look up the URL from a given book because you don't have to remember the exact position in the list but can simply pass it the title of the book. \n",
        "\n",
        "In the Google Inspector screenshot at the beginning of this section, you can see that the book title is stored in the `alt` attribute of the `<img>` tag (as well as in the `title` attribute from the second `<a>` tag). Using a similar approach as above, we collect the `book_title` and `book_url` of each book, and use these records to update `book_list`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihnteFpzR6YX"
      },
      "outputs": [],
      "source": [
        "book_list = []\n",
        "\n",
        "for book in books: \n",
        "    book_title = book.find(\"img\").attrs[\"alt\"] \n",
        "    book_url = book.find(\"a\").attrs[\"href\"]\n",
        "    book_list.append({'title': book_title,\n",
        "                      'url': book_url})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKVCbXXCR6YY"
      },
      "source": [
        "As a result, we can simply pass the book title (mind the capitals!) to the following code snippet to obtain the corresponding URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Pu-L9pj-R6YY"
      },
      "outputs": [],
      "source": [
        "next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf2qW_zLR6YY"
      },
      "source": [
        "#### Exercise 3\n",
        "1. Like exercise 2.2, write code that transforms the relative URLs (`../..`) in `book_list` into full URLs, stored in `full_url`. Tip: you can use `for id, book in enumerate(book_list):` to iterate over the dictionaries and update URLs accordingly. \n",
        "2. One of the books on `books.toscrape.com` is [Black Dust](https://books.toscrape.com/catalogue/black-dust_976/index.html). What happens once you search for it using the code snippet above? Why is that? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHHPWLBQR6YY"
      },
      "outputs": [],
      "source": [
        "# your answer goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9KjDiYzR6YZ"
      },
      "source": [
        "#### Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNhh1N8AR6YZ"
      },
      "outputs": [],
      "source": [
        "# Question 1\n",
        "for id, book in enumerate(book_list):\n",
        "    book[\"full_url\"] = (base_url + book[\"url\"]).replace('../','')\n",
        "\n",
        "# show the first five elements\n",
        "book_list[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "kGeAgVo7R6YZ"
      },
      "outputs": [],
      "source": [
        "# Question 2 \n",
        "next((book for book in book_list if book[\"title\"] == \"Black Dust\"), None)\n",
        "\n",
        "# it does not return any result because the book does not exist (this book is on shown on the 2nd page and we only scraped the first one!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dkjCc8jR6Ya"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqg6yltiR6Ya"
      },
      "source": [
        "### 1.4 Page Navigation\n",
        "\n",
        "__Importance__\n",
        "\n",
        "Alright - what have we learnt up this point?\n",
        "\n",
        "- Section 1.1 taught us how to extract links from a page, \n",
        "- Section 1.2 taught us how to extract *more specific links* from a page, and finally\n",
        "- Section 1.3 taught us how to assemble a list of *links* to *all* books listed on a specific page.\n",
        "\n",
        "So... what's missing?\n",
        "\n",
        "Exactly! The [`books.toscrape.com`](https://books.toscrape.com/catalogue/category/books_1/index.html) contains __1000 books__, spread across __50 pages__. \n",
        "\n",
        "So, the goal of this section is to navigate through the __entire book assortment__, not only the first 20 books.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTDTPExRR6Ya"
      },
      "source": [
        "__Let's try it out__\n",
        "\n",
        "Open [the website](https://books.toscrape.com/catalogue/category/books_1/index.html), and click on the \"next\" button at the bottom of the page.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/books.png\" align=\"left\" width=60%/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeKM6WUvR6Ya"
      },
      "source": [
        "\n",
        "Repeat this a couple of times, and observe how the URL in your navigation bar is changing...\n",
        "\n",
        "- `https://books.toscrape.com/catalogue/category/books_1/page-1.html`\n",
        "- `https://books.toscrape.com/catalogue/category/books_1/page-2.html`\n",
        "- `https://books.toscrape.com/catalogue/category/books_1/page-3.html`\n",
        "\n",
        "Can you guess the next one...?\n",
        "\n",
        "Indeed! The URL can be divided into a __fixed base part__ (`https://books.toscrape.com/catalogue/category/books_1/`), and a __counter__ that is dependent on the page you're visiting (e.g., `page-1.html`). \n",
        "\n",
        "__Now let's create a list of all 50 URLs!__ \n",
        "\n",
        "First, we create a counter variable, which we now set to 1 (but it can take on any value later on). Then, we concatenate the `base_url` with the counter (note that we have to convert the integer counter to a string before we can do that, using the `str` function)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpUQ1MWKR6Yb"
      },
      "outputs": [],
      "source": [
        "counter = 1\n",
        "full_url = base_url + \"page-\" + str(counter) + \".html\" \n",
        "print(full_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGyfEvQuR6Yb"
      },
      "source": [
        "In a similar fashion, we generate a list of 50 `page_urls` with a for loop that starts at 1 and ends at 50 (not 51!). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddjIdV9VR6Yb"
      },
      "outputs": [],
      "source": [
        "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
        "page_urls = []\n",
        "\n",
        "for counter in range(1, 51):\n",
        "    full_url = base_url + \"page-\" + str(counter) + \".html\" \n",
        "    page_urls.append(full_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKvZn9xWR6Yc"
      },
      "source": [
        "As expected, this gives a list of all page URLs that contain books. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVsDTVWZR6Yi"
      },
      "outputs": [],
      "source": [
        "# print the last five page urls (btw, run print(page_urls) for yourself to see all page URLs!)\n",
        "print(\"The number of page urls in the list is: \" + str(len(page_urls)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJkf23IYR6Yi"
      },
      "source": [
        "#### Exercise 4\n",
        "In this exercise, we practice generating a seed for another website, [`quotes.toscrape.com`](https://quotes.toscrape.com/), which displays 100 famous quotes from GoodReads, categorized by tag. \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/quotes.png\" align=\"left\" width=60% style=\"border: 1px solid black\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SHYNpVgR6Yi"
      },
      "source": [
        "1. Make yourself comfortable with how the [site](https://quotes.toscrape.com) works and ask yourself questions such as: how does the navigation work, how many pages are there, what is the base URL, and how does it change if I move to the next page?\n",
        "2. Generate a list `quote_page_urls` that contains the page URLs we need if we'd like to scrape all 100 quotes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxYPwnhrR6Yj"
      },
      "outputs": [],
      "source": [
        "# your answer goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gkep9FoR6Yj"
      },
      "source": [
        "#### Solutions\n",
        "1. The 100 quotes are evenly spread across 10 pages. The base URL is `https://quotes.toscrape.com/page/` followed by a page number between 1 and 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8-7UrFLR6Yj"
      },
      "outputs": [],
      "source": [
        "# Question 2\n",
        "base_url = \"https://quotes.toscrape.com/page/\"\n",
        "quote_page_urls = []\n",
        "\n",
        "for counter in range(1, 11):\n",
        "    full_url = base_url + str(counter)\n",
        "    quote_page_urls.append(full_url)\n",
        "\n",
        "print(quote_page_urls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL7qiFzdR6Yk"
      },
      "source": [
        "### 1.4 Wrap-Up\n",
        "In summary, we have defined our seed and thought about a data extraction strategy to obtain the book links on a page. Since there are multiple pages, we needed to generate a list of URLs as an input for our scraper, which we'll further refine in the next chapter.  \n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M27m32GR6Yk"
      },
      "source": [
        "## 2. Data Extraction\n",
        "\n",
        "\n",
        "### 2.1 Timers\n",
        "\n",
        "__Importance__\n",
        "\n",
        "Before we start running the scraper, we need to realize that sending many requests at the same time can overload a server. Therefore, it's highly recommended to pause between requests rather than sending them all simultaneously. This avoids that your IP address (i.e., numerical label assigned to each device connected to the internet) gets blocked, and you can no longer visit (and scrape) the website. \n",
        "\n",
        "__Let's try it out__\n",
        "\n",
        "In Python, you can import the `sleep` module, which pauses the execution of future commands for a given amount of time. For example, the print statement after `sleep(5)` will only be executed after 5 seconds:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Riftpk_eR6Yk"
      },
      "outputs": [],
      "source": [
        "# run this cell again to see the timer in action yourself!\n",
        "from time import sleep\n",
        "sleep(5)\n",
        "print(\"I'll be printed to the console after 5 seconds!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "158JgG2QR6Yk"
      },
      "source": [
        "__Exercise 5__\n",
        "\n",
        "Modify the code above to sleep for 2 minutes. Go grab a coffee inbetween. Did it take you longer than 2 minutes?\n",
        "\n",
        "(if you want to abort the running code, just select the cell and push the \"stop\" button)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWfTWfR1R6Yk"
      },
      "outputs": [],
      "source": [
        "# your answer goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-vWqIBaR6Yk"
      },
      "source": [
        "**Solution**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mu_FfrlwR6Yl"
      },
      "outputs": [],
      "source": [
        "sleep(2*60)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXFfsZKnR6Yl"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMgPqmcbR6Yl"
      },
      "source": [
        "### 2.2 Modularization\n",
        "\n",
        "**Importance**  \n",
        "\n",
        "In scraping, many things have to be executed *multiple times*. For example, whenever we open a new page with books, we would like to extract all the available book links.\n",
        "\n",
        "To help us execute things over and over again, we will \"modularize\" our code into functions. We can then call these functions whenever we need them. Another benefit from using functions is that we can improve the readability and reusability of our code. If you need a quick refresher on functions, please revisit section 4 of the [Python Bootcamp](https://odcm.hannesdatta.com/docs/tutorials/pythonbootcamp/) tutorial.\n",
        "\n",
        "**Let's try it out**\n",
        "\n",
        "Let's finish up our book URL scraper by putting together everything we have learned thus far.\n",
        "\n",
        "First, we define a function `generate_page_urls()` that takes a base URL and an upper limit of the number of pages (`num_pages`) as input parameters. This way, we can easily update our scraper if more books are added or if the base URL changes (e.g., change `num_pages` from `5` to `6` if we also want to include the 6th page). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2dThnYwR6Yl"
      },
      "outputs": [],
      "source": [
        "def generate_page_urls(base_url, num_pages):\n",
        "    '''generate a list of full page urls from a base url and counter that has takes on the values between 1 and num_pages'''\n",
        "    page_urls = []\n",
        "    \n",
        "    for counter in range(1, num_pages + 1):\n",
        "        full_url = base_url + \"page-\" + str(counter) + \".html\"\n",
        "        page_urls.append(full_url)\n",
        "        \n",
        "    return page_urls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlAdcxJZR6Yl"
      },
      "source": [
        "Try running the function and modifying its parameters (e.g., set the number of book pages to `10` rather than `5`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI36lEtPR6Yl"
      },
      "outputs": [],
      "source": [
        "generate_page_urls(\"https://books.toscrape.com/catalogue/category/books_1/\", 5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_acuMfXR6Yl"
      },
      "source": [
        "Second, let's define an `extract_book_urls()` function, which takes a list of page URLs (`page_urls`; like the one above!) as input and returns a list of dictionaries with book titles and URLs. Note the two-step structure of the for-loops: on every page (`page_url`), we create a `books` object which we subsequently loop over by extracting the `book_title` (e.g., `A Light in the Attic`) and `book_url` (e.g., `https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html`) from each book. These records are added to the list `book_list` which is eventually returned by the function. Make sure to fully understand this function line by line before moving on!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uh_IfgibR6Ym"
      },
      "outputs": [],
      "source": [
        "def extract_book_urls(page_urls):\n",
        "    '''collect the book title and url for every book on all page urls'''\n",
        "    book_list = []\n",
        "    \n",
        "    # collect all books on page_url\n",
        "    for page_url in page_urls: \n",
        "        res = requests.get(page_url)\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "        books = soup.find_all(class_=\"product_pod\")\n",
        "        \n",
        "        # for each book on that page look up the title and url and store it in a list\n",
        "        for book in books: \n",
        "            book_title = book.find(\"img\").attrs[\"alt\"] \n",
        "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
        "            book_list.append({\"title\": book_title,\n",
        "                             \"url\": book_url}) \n",
        "            \n",
        "        sleep(1)  # pause 1 second after each request\n",
        "            \n",
        "    return book_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9-ejRKuR6Ym"
      },
      "source": [
        "So, let's try out this function. Be aware that running it takes some time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIOJugOjR6Yn"
      },
      "outputs": [],
      "source": [
        "# this cell references functions in other cells, therefore make sure you have loaded all cells above first! (Cell > Run All Above)\n",
        "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
        "page_urls = generate_page_urls(base_url, 2) # to save time and resources we only scrape the first 2 pages\n",
        "book_list = extract_book_urls(page_urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Gme6HxIR6Yo"
      },
      "outputs": [],
      "source": [
        "# Preview the results\n",
        "book_list[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmh3Gxq8R6Yo"
      },
      "source": [
        "__Exercise 6__\n",
        "\n",
        "1. Please obtain a list of URLs for products stored on the first *five* pages. \n",
        "2. Please extend the `extract_books_url` function to also obtain information on whether the book is in stock. Make use of this code snippet to search for the particular class: `book.find(\"p\", class_=\"class-name-to-search-for\")`\n",
        "3. Please clean the text snippet obtained in 2 by removing (a) the unnecessary line breaks (`\\n`), and spaces (`\" \"`), using Python's `replace` function. Finally, test your function!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L66MtfEuR6Yo"
      },
      "outputs": [],
      "source": [
        "# Your answer goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SenS3-iGR6Yo"
      },
      "source": [
        "**Solutions**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnpUqprYR6Yo"
      },
      "outputs": [],
      "source": [
        "# Question 1\n",
        "\n",
        "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
        "page_urls = generate_page_urls(base_url, 5) \n",
        "book_list = extract_book_urls(page_urls)\n",
        "book_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM0ng9EDR6Yp"
      },
      "outputs": [],
      "source": [
        "# Question 2\n",
        "def extract_book_urls(page_urls):\n",
        "    '''collect the book title and url for every book on all page urls'''\n",
        "    book_list = []\n",
        "    \n",
        "    # this part is the same as above\n",
        "    for page_url in page_urls: \n",
        "        res = requests.get(page_url)\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "        books = soup.find_all(class_=\"product_pod\")\n",
        "\n",
        "        for book in books: \n",
        "            book_title = book.find(\"img\").attrs[\"alt\"] \n",
        "            book_url = (\"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"]).replace('../','')\n",
        "            book_instock = book.find(\"p\", class_=\"instock availability\").text # only this changed!\n",
        "            book_list.append({\"title\": book_title,\n",
        "                             \"url\": book_url,\n",
        "                             \"instock\": book_instock}) # and this line!\n",
        "            \n",
        "        sleep(1)  \n",
        "            \n",
        "    return book_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suHbIJbDR6Yp"
      },
      "outputs": [],
      "source": [
        "# Question 3\n",
        "def extract_book_urls(page_urls):\n",
        "    '''collect the book title and url for every book on all page urls'''\n",
        "    book_list = []\n",
        "    \n",
        "    for page_url in page_urls: \n",
        "        res = requests.get(page_url)\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "        books = soup.find_all(class_=\"product_pod\")\n",
        "\n",
        "        for book in books: \n",
        "            book_title = book.find(\"img\").attrs[\"alt\"] \n",
        "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
        "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
        "            \n",
        "            # addition to clean up the text (the rest remains the same!)\n",
        "            book_instock = book_instock.replace('\\n','').replace(' ','') # first replace a line-break (`\\n`) by an empty space, then replace a space (' ') by an empty space\n",
        "            \n",
        "            book_list.append({\"title\": book_title,\n",
        "                             \"url\": book_url,\n",
        "                             \"instock\": book_instock})\n",
        "            \n",
        "        sleep(1) \n",
        "            \n",
        "    return book_list\n",
        "\n",
        "# test function!\n",
        "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
        "page_urls = generate_page_urls(base_url, 2) \n",
        "book_list = extract_book_urls(page_urls)\n",
        "book_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRvfaXVnR6Yp"
      },
      "source": [
        "---\n",
        "\n",
        "### 2.3 Next Page Button\n",
        "\n",
        "__Importance__\n",
        "\n",
        "For now, the book link extraction has worked without problems. Yet ,there's still one little improvement that we can make. *If the number of pages changes*, we need to manually update the `num_pages` parameter. For example, we may miss out once new books are added which appear on page 51 and further. \n",
        "\n",
        "A general solution is therefore to look up whether there is a `next` button on the page (HTML code below). If so, it means a next page exists, and we keep on incrementing the page counter by 1. If not, it means we have reached the last page. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzweh7mwR6Yq"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/next_page.png\" align=\"left\" width=60% style=\"border: 1px solid black\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS5pwh5FR6Yq"
      },
      "source": [
        "__Let's try it out__\n",
        "\n",
        "So, let's write a function (`check_next_page()`), which takes an URL as an input and returns the outgoing link of the next button (if present):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVT56dLJR6Yq"
      },
      "outputs": [],
      "source": [
        "def check_next_page(url):\n",
        "    res = requests.get(url)\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "    next_btn = soup.find(class_= \"next\") # observe the similarity with the code snippet used above\n",
        "    return next_btn.find(\"a\").attrs[\"href\"] if next_btn else None\n",
        "\n",
        "page_1 = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
        "print(\"The next page is: \" + check_next_page(page_1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUrmLxjOR6Yq"
      },
      "source": [
        "#### Exercise 7\n",
        "1. Pass `https://books.toscrape.com/catalogue/page-50.html` to `check_next_page()` and observe the output. Is that what you expected? \n",
        "2. Write a function `next_page_url()` that that checks whether the output of `check_next_page()` is not equal to `None` (i.e., anything but `None`). If so, it should return a new variable `page_url` that concatenates the base URL and the relative path to the next page. If not, it should print the statement `This is already the last page!`. Tip: make use of `if`/`else` statements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08pwsZFiR6Yr"
      },
      "outputs": [],
      "source": [
        "# your answer goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOFDNI5CR6Yr"
      },
      "source": [
        "#### Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2sJIRPCR6Yr"
      },
      "outputs": [],
      "source": [
        "# Question 1 \n",
        "output = check_next_page(\"https://books.toscrape.com/catalogue/page-50.html\")\n",
        "print(output) # the output is None because page 50 is the last one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "_dZaJj-rR6Yr"
      },
      "outputs": [],
      "source": [
        "# Question 2 \n",
        "def next_page_url(url):\n",
        "    base_url = \"https://books.toscrape.com/catalogue/\"\n",
        "    if url != None: \n",
        "        page_url = base_url + url \n",
        "        return page_url \n",
        "    else: \n",
        "        print(\"This is already the last page!\")\n",
        "        \n",
        "next_page_url(check_next_page(\"https://books.toscrape.com/catalogue/page-50.html\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzyl-0lbR6Yr"
      },
      "source": [
        "---\n",
        "### 2.4 Combining everything in one function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nSfXfVgR6Ys"
      },
      "source": [
        "__Importance__\n",
        "\n",
        "Our scraper so far consists of a function that extracts books from the page (`extract_books_urls()`), a function to check whether a next page is available (`next_page_url`), and a function that looks up the next URL (`check_next_page()`). \n",
        "\n",
        "As a last step, we can now integrate these functions into one *overarching* function. Instead of generating the list of page URLs up front, we use a `while` loop that remains `True` as long as there is another new page. At the end of each loop, we update the `page_url` according to the link of the next button (using `check_next_page()`). On the last page, there is no new page URL and thus we break out of the while loop. We've added a print statement at the beginning of the `while` loop, so that you can observe the progress of the scraper while it is running.\n",
        "\n",
        "All in all, we have modularized our code into functions, made it future-proof (e.g. if new books are added), and reduced the number of lines of code to get the job done! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKOe1kffR6Ys"
      },
      "outputs": [],
      "source": [
        "def extract_all_books(page_url):\n",
        "    books = []\n",
        "    while page_url:\n",
        "        print(page_url)\n",
        "        for book in extract_book_urls([page_url]):\n",
        "            books.append(book)\n",
        "        \n",
        "        if check_next_page(page_url) != None: \n",
        "            page_url = \"https://books.toscrape.com/catalogue/category/books_1/\" + check_next_page(page_url)\n",
        "        else: \n",
        "            break\n",
        "        \n",
        "        # if \"page-4\" in page_url: break # (activate this if you don't want to run the entire loop)\n",
        "    return books"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOGyhU2qR6Ys"
      },
      "source": [
        "__Let's try it out__\n",
        "\n",
        "Run the cell below to see the scraper in action! You may need to wait for a bit as the scraper loops through all 50 pages. If you don't feel like taking a coffee break, remove the `#` sign in front of the `if` statement to abort the function before it starts with page 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W29pivE3R6Ys"
      },
      "outputs": [],
      "source": [
        "book_list = extract_all_books(\"https://books.toscrape.com/catalogue/page-1.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcNFnWC8R6Yt"
      },
      "outputs": [],
      "source": [
        "book_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ_8NHNsR6Yz"
      },
      "source": [
        "#### Exercise 8\n",
        "\n",
        "After having run the scraper above, inspect the output yourself, and then answer the following questions.\n",
        "\n",
        "1. How many books are there in `books`? Does this align with your initial expectations?\n",
        "2. A friend recommended the book `The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics`. After looking up the reviews on [GoodReads](https://www.goodreads.com/book/show/25986790-the-activist-s-tao-te-ching?ac=1&from_search=true&qid=jpcvOsxKfP&rank=1), you decide to look for a copy of the book online. Does [books.toscrape.com](books.toscrape.com) offer a copy in their store? If so, do they have enough stock currently?\n",
        "3. How many books are in stock currently?\n",
        "4. How many books are there that have the word \"boat\" (lower or upper case) in their title?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1Elqi-vR6Y1"
      },
      "outputs": [],
      "source": [
        "# your answer goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy-6cPT3R6Y2"
      },
      "source": [
        "#### Solutions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aztK9C9QR6Y4"
      },
      "outputs": [],
      "source": [
        "# Question 1\n",
        "# There are 1000 books \n",
        "\n",
        "len(books)\n",
        "\n",
        "#That's 50 pages into 20 products, which matches our expectations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CYBL3uWR6Y4"
      },
      "outputs": [],
      "source": [
        "books[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q79w3fsUR6Y5"
      },
      "outputs": [],
      "source": [
        "# Question 2\n",
        "# we use one of the code snippets from above to search for the title\n",
        "\n",
        "next((book for book in book_list if book[\"title\"] == 'The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics'), None)\n",
        "\n",
        "# we can view the URL and open it in the browser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSYSQ9SgR6Y6"
      },
      "outputs": [],
      "source": [
        "# Question 3\n",
        "books_instock = [book for book in book_list if book[\"instock\"] == \"Instock\"]\n",
        "len(books_instock)\n",
        "\n",
        "# All books are in stock!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpR5pSmcR6Y6"
      },
      "outputs": [],
      "source": [
        "# Question 4\n",
        "len([book for book in book_list if \"boat\" in book[\"title\"].lower()])\n",
        "\n",
        "# here, we're checking for the appearence of the word \"boat\" in the title."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8qHbJHlR6Y7"
      },
      "source": [
        "In case you haven't done so, it's time to take a break now! Enjoy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vlxd_7EIR6Y7"
      },
      "source": [
        "---\n",
        "\n",
        "### 2.5 Page-Level Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrTViwtsR6Y7"
      },
      "source": [
        "**Importance**   \n",
        "Do you remember trying to obtain the URL of the [Black Bust](https://books.toscrape.com/catalogue/black-dust_976/index.html) book in exercise 2? Let's see whether it works this time... (you have to run the entire code above for 50 pages!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoFVXeZZR6Y7"
      },
      "outputs": [],
      "source": [
        "[book for book in book_list if book[\"title\"] == \"Black Dust\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sysaMXbcR6Y7"
      },
      "source": [
        "Excellent, it works flawlessly! But, why did we need the book URLs in the first place? It forms the seed for other web scraping efforts. For example, the product descriptions can only be obtained from the book pages themselves which means we need to loop over all book URLs to extract the right information. \n",
        "\n",
        "**Let's try it out!**  \n",
        "\n",
        "In the follow-up exercise, we'll look at how to do this. So... open the [website](https://books.toscrape.com/catalogue/black-dust_976/index.html) in your browser, and run the code cell below to extract the number of reviews for that particular book."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkS2B0eUR6Y8"
      },
      "outputs": [],
      "source": [
        "res = requests.get('https://books.toscrape.com/catalogue/black-dust_976/index.html')\n",
        "soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "len(soup.find(id=\"content_inner\").find(\"p\", class_ = \"star-rating\").find_all(class_ = \"icon-star\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11iZHz5xR6Y8"
      },
      "source": [
        "After running the cell, inspect the website's source code in Chrome, and try to understand the extraction code above. A good way to do so is to break down your extraction code in small chunks, and run them after another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J8bzxSGR6Y8"
      },
      "outputs": [],
      "source": [
        "soup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzy4OEc-R6Y9"
      },
      "source": [
        "This gave you the entire source code of the website - not so useful as a starting point, so let's zoom in on the what is labeled in the source as the \"Start of product page\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjFyjUFlR6Y9"
      },
      "outputs": [],
      "source": [
        "soup.find(id=\"content_inner\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFehRt44R6Y9"
      },
      "source": [
        "This one already looks better, scrolling down just a little bit gives us already the title and price of the product. Can you find this information also in Chrome's Inspector Tool?\n",
        "\n",
        "Let's proceed by zooming in even more..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqsSM5rKR6ZL"
      },
      "outputs": [],
      "source": [
        "soup.find(id=\"content_inner\").find_all(\"p\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGKh4Ti3R6Zb"
      },
      "source": [
        "We've now filtered for all content items with tag `p`, and can spot the target class: \"star-rating\"! So let's go there..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cauYbZwqR6Zc"
      },
      "outputs": [],
      "source": [
        "soup.find(id=\"content_inner\").find_all(\"p\", class_ = \"star-rating\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BGzvKNiR6Ze"
      },
      "source": [
        "Wow - so many star ratings! The list contains the star rating of the product (overall), and the reviewer's individual star rankings. Let's just extract the first star rating for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UXFTeTJR6Zh"
      },
      "outputs": [],
      "source": [
        "soup.find(id=\"content_inner\").find(\"p\", class_ = \"star-rating\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kg7ijjTR6Zi"
      },
      "source": [
        "Much better. But... where can we see the number of stars? It's in the class name (\"star-rating Five\"), but we can also just count the number of \"icon-star\" classes in the code above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIV0wieRR6Zj"
      },
      "outputs": [],
      "source": [
        "soup.find(id=\"content_inner\").find(\"p\", class_ = \"star-rating\").find_all(class_ = \"icon-star\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8FVMSb1R6Zj"
      },
      "source": [
        "The last thing to do is to count how many items are in that class, by using the `len` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpDrikz-R6Zj"
      },
      "outputs": [],
      "source": [
        "len(soup.find(id=\"content_inner\").find(\"p\", class_ = \"star-rating\").find_all(class_ = \"icon-star\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fZ2-hnlR6Zk"
      },
      "source": [
        "What a journey! We hope you enjoyed this little exploration activity, and are ready for the next exercisese."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-T_dM2hR6Zk"
      },
      "source": [
        "#### Exercise 9\n",
        "1. Please write a function `get_book_description` to extract the product description for the first five books in `book_list`.  \n",
        "\n",
        "2. Run the function and inspect the output. If you look carefully, you may spot `tÃ©gÃ©` symbols throughout the product description. Look up the original text on the book pages and compare it side-by-side with the output below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFCL0jVQR6Zl"
      },
      "outputs": [],
      "source": [
        "# your answer goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us7Fg96lR6Zl"
      },
      "source": [
        "#### Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmFCyD0pR6Zm"
      },
      "outputs": [],
      "source": [
        "# Question 1\n",
        "def get_book_description(books):\n",
        "    book_descriptions = []\n",
        "    \n",
        "    for book in books: \n",
        "        page_url = book[\"url\"]\n",
        "\n",
        "        res = requests.get(page_url)\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "        # tip: look at the Google Inspector screenshot below \n",
        "        description = soup.find(id=\"content_inner\").find_all(\"p\")[3].get_text()\n",
        "        title = soup.find(id=\"content_inner\").find('img')['alt']\n",
        "        book_descriptions.append({'url': page_url,\n",
        "                                  'title': title,\n",
        "                                  'description': description})\n",
        "    return book_descriptions\n",
        "\n",
        "book_descriptions = get_book_description(book_list[0:5])\n",
        "book_descriptions\n",
        "\n",
        "# Question 2\n",
        "# tÃ©gÃ© (or similarly encoded strings) are characters from languages other than English, which use an extended character space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVveaHANR6Zm"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/black_dust.png\" align=\"left\" width=70% style=\"border: 1px solid black\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9qHsz4oR6Zm"
      },
      "source": [
        "---\n",
        "### 2.6 Scraping to a CSV file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYDS1rvSR6Zn"
      },
      "source": [
        "**Importance**  \n",
        "Lastly, we convert the list of dictionaries into a Comma Separated Values (CSV) file, which you can open up in any spreadsheet program (e.g., Excel). \n",
        "\n",
        "More specifically, we'd like to have a file with three columns, containing:\n",
        "- the book title, \n",
        "- the product description, and \n",
        "- the current date and time. \n",
        "\n",
        "The latter helps you to distinguish between data from scrapers you run repeatedly. For example, you may run the book scraper at the beginning of every month to keep track of price changes of any of the books. Although you could store the data of each extraction moment into a separate file (e.g., `2021_01_01_book_prices.csv` for January 2021, `2021_02_01_book_prices.csv` for February 2021), we recommend always including a timestamp column to your scraped datasets. After all, losing or overwriting data can be disastrous (especially for scrapers) as you may never be able to obtain historical data (e.g., the price of a book 2 months ago)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD-yv2QfR6Zo"
      },
      "source": [
        "In that light, we import the `datetime` library which contains a function `now()` that automatically determines the current date and time which we'll incorporate into our final dataset. Run the cell a few times, and observe how the values update to the current time: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJq6UKhjR6Zo"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "print(now)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSE_T9q1R6Zp"
      },
      "source": [
        "In essence, CSV-files are simply text files with symbols that indicate the beginning of a new column (i.e., delimiter). Below you find a screenshot of the `book_descriptions.csv` file opened in a basic text editor. Every `;` and enter (empty line) indicate the start of a new column and row, respectively.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/csv_files.png\" align=\"left\" width=50% style=\"border: 1px solid black\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlP6MFGMR6Zp"
      },
      "source": [
        "Excel then applies this logic - converting semicolons and empty lines - to assign the data points to their respective cells: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xWXCLpiR6Zp"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/excel.png\" align=\"left\" width=50% style=\"border: 1px solid black\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol9UUAVRR6Zq"
      },
      "source": [
        "It gets more complicated once the delimiter has been embodied into data. For example, a comma is sometimes also used as a delimiter, but that would not work here because the product description also contains commas (e.g., `No matter how busy he keeps himself, successful Broadway...`). In that case, the part after the comma (`successful Broadway...`) would be regarded as a new column, whereas it actually still belongs to the product description. For that reason, setting the delimiter to `;` is a safer choice here. In practice, tabs \"\\t\" are also frequently used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ1uwtQFR6Zq"
      },
      "source": [
        "**Let's try it out!**   \n",
        "We can write to a text file with the `csv` library. The first row is the header and contains the three column names (`\"title\", \"description\", \"date_time\"`). Thereafter, we iterate over the list and add the current date time to it. Importantly, the `w` flag in the `with` statement indicates that the file will be overwritten every time the cell is executed. If you, however, want to append data to an existing file and avoid losing historical data, you can swap `w` for `a`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D0JuiOZR6Zq"
      },
      "outputs": [],
      "source": [
        "import csv \n",
        "\n",
        "with open(\"book_descriptions.csv\", \"w\", encoding = 'utf-8') as csv_file: # <<- this is the line with the \"flag\"l see exercises below\n",
        "    writer = csv.writer(csv_file, delimiter = \";\")\n",
        "    writer.writerow([\"title\", \"description\", \"date_time\"])\n",
        "    now = datetime.now()\n",
        "    for book in book_descriptions: # here we reference the book_descriptions list - make sure it's loaded otherwise you get an error! (Cell > Run All Above)\n",
        "        writer.writerow([book['title'], book['description'], now])\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs1tR59tR6Zr"
      },
      "source": [
        "#### Exercise 10\n",
        "1. Run the cell above and look at the `book_descriptions.csv` file in Excel. Make sure it looks like the screenshot above (3 columns x 4 rows). Depending on the language settings on your machine, the data may not be correctly distributed over the columns. In that case, go to the \"Data\" tab in Excel, click the \"Text to Columns\" button in the ribbon, choose \"Delimited\", put a checkmark in front of \"Semicolon\", and choose \"Finish\".\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/webscraping101/images/text_to_column.gif\" align=\"left\" width=60% style=\"border: 1px solid black\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILRg_626R6Zr"
      },
      "source": [
        "2. Close Excel, change the flag to `a`, and run the cell again. Open the `book_descriptions.csv` file again (and repeat the Text to Columns procedure if necessary). How does the output differ from the previous step? Why is that? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noBLX-LZR6Zs"
      },
      "outputs": [],
      "source": [
        "# your answer goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGheiFWIR6Zu"
      },
      "source": [
        "#### Solutions  \n",
        "It shows the same data, including the header, twice (below one another). It goes beyond the scope of this course to define better alternatives (e.g., save data to a database).\n",
        "\n",
        "---\n",
        "\n",
        "### 2.8 Wrap-Up\n",
        "At the beginning of this tutorial, we set out the promise of writing multi-page scrapers from start to finish. Although the examples we have studied are relatively simple, the same principles (seed definition, data extraction plan, page-level data collection) apply to any other website you'd like to scrape. \n",
        "\n",
        "Now that you have hopefully got the hang of using Jupyter Notebooks, we're going to introduce you to an alternative that goes hand in hand with what you have learned thus far, but overcomes some of its limitations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_wQC7nrR6Zv"
      },
      "source": [
        "## 3. Executing Python Files\n",
        "\n",
        "### 3.1 Jupyter Notebooks versus Spyder\n",
        "\n",
        "Jupyter Notebooks are ideal for combining programming and markdown (e.g., text, plots, equations), making it the default choice for sharing and presenting reproducible data analyses. Since we can execute code blocks one by one, it's suitable for developing and debugging code on the fly. \n",
        "\n",
        "That said, Jupyter Notebooks also have some severe limitations when using them in production environments. That's where an \"Integrated Development Environment\" (IDE) comes in, such as Spyder or PyCharm. A fancy word, we know. So, let's revisit the most important differences.\n",
        "\n",
        "First, the order in which you run cells within a notebook may affect the results. While prototyping, you may lose sight of the top-down hierarchy, which can cause problems once you restart the kernel (e.g., a library is imported after it is being used). Second, there is no easy way to browse through directories and files within a Jupyter Notebook. Third, notebooks cannot handle large codebases nor big data remarkably well. \n",
        "\n",
        "That's why we recommend starting in Jupyter Notebooks, moving code into functions along the way, and once all seems to be running well, copy-paste all necessary code into Spyder. From there, you can save it as a Python file (`.py`) - rather than a notebook (`.ipynb`) - and execute the file from the command line. In this tutorial, we introduce you to the Spyder IDE and learn how to run Python files from the command line. The reason we choose for the Spyder IDE instead of PyCharm, for example, is because Spyder is already installed with Anaconda. In the future, you can always use PyCharm or another text editor to write your python scripts if you prefer! \n",
        "\n",
        "### 3.2 Introduction to Spyder\n",
        "The first time you need to click on the green \"Install\" button in Anaconda Navigator, after which you start Spyder by clicking on the blue \"Launch\" button (alternatively, type `spyder` in the terminal). \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/anaconda_navigator.png\" width=90% align=\"left\" style=\"border: 1px solid black\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRn6qq_wR6Z2"
      },
      "source": [
        "The main interface consists of three panels: \n",
        "1. **Code editor** = where you write Python code (i.e., the content of code cells in a notebook)\n",
        "2. **Variable / files** = depending on which tab you choose either an overview of all declared variables (e.g. look up their type or change their values) or a file explorer (e.g., to open other Python files)\n",
        "3. **Console** = the output of running the Python script from the code editor (what normally appears below each cell in a notebook)\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/spyder.png\" width=90% align=\"left\" style=\"border: 1px solid black\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUi6mYSPR6Z2"
      },
      "source": [
        "**Let's try it out!**     \n",
        "In the `webscraping_101.py` file above, we have put together all code snippets from this notebook needed to scrape and store the URLs of all books. To run the script you either click on the green play button to run all code (from line 1 to 46). As an alternative, you can highlight the parts of the script you want to execute and then click the run selection button.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/toolbar.png\" width=40% align=\"left\" style=\"border: 1px solid black\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAdtoMWMR6Z3"
      },
      "source": [
        "Once the script is running, you may need to interrupt the execution because it is simply taking too long or you spotted a bug somewhere. Click on the red rectangular in the console to stop the execution. \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/interrupt.gif\" width=80% align=\"left\" style=\"border: 1px solid black\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bfYozg0R6Z4"
      },
      "source": [
        "#### Exercise 11\n",
        "1. Download the Python [webscraping_101.py](https://odcm.hannesdatta.com/docs/tutorials/webscraping101/webscraping_101.py) script (right-click, download linked file as…) and store it in the same directory as the `.ipynb` notebook file (`py` = Python script; `.ipynb` = Jupyter notebook). \n",
        "1. Start Spyder  and open the `webscraping101.py` (`File` > `Open`) script (so not the notebook!). Compare this notebook and the Python script in Spyder side-by-side: which do you find clearer? \n",
        "2. Run the script and then open the `book_urls.csv` file in Excel. Where is the file stored on your computer? How many records are there?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-v6vnzPR6Z5"
      },
      "outputs": [],
      "source": [
        "# your answer goes here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnTshkvFR6Z6"
      },
      "source": [
        "#### Solutions\n",
        "1. It remains a personal opinion, but we'd say the `.py` looks neater because all the code is in the same view (e.g., all import statements below each other rather than spreading them throughout your notebook)\n",
        "2. Exported files appear in the same working directory (unless specified differently). The `book_urls.csv` file contains 1000 rows (999 records and 1 header row)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru8OrO5nR6Z6"
      },
      "source": [
        "### 3.3 Run Python Files \n",
        "* *Mac*\n",
        "    1. Open the terminal and navigate to the folder in which the `.py` file has been saved (use `cd` to change directories and `ls` to list all files).\n",
        "    2. Run the Python script by typing `python <FILENAME.py>` (e.g., `python webscraping_101.py`).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/dev/content/docs/tutorials/webscraping101/images/running_python.gif\" width=60% align=\"left\" style=\"border: 1px solid black\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmw4XJA9R6Z6"
      },
      "source": [
        "* *Windows*\n",
        "    1. Open Windows explorer and navigate to the folder in which the `.py` file has been saved. Type `cmd` to open the command prompt. Alternatively, open the command prompt from the start menu (and use `cd` to change directories and `dir` to list files).\n",
        "    2. Activate Anaconda by typing `conda activate`.\n",
        "    3. Run the Python script by typing `python <FILENAME.py>` (e.g., `python webscraping_101.py`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-7VkMw-R6Z7"
      },
      "source": [
        "### 3.4 Wrap-up\n",
        "\n",
        "Congrats! You've made it, and learnt so much. Take a step back now, let it sink in, and then get creative on how you could use the skills you've learnt. \n",
        "\n",
        "This is the end of this tutorial. Keep up the good work!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "webscraping-101.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}